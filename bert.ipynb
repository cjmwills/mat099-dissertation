{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 751kB/s] \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 16.0kB/s]\n",
      "Downloading: 100%|██████████| 152/152 [00:00<00:00, 31.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ydshieh/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 694/694 [00:00<00:00, 78.3kB/s]\n",
      "Downloading: 100%|██████████| 411M/411M [01:06<00:00, 6.53MB/s] \n",
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at ydshieh/bert-base-cased-squad2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForQuestionAnswering.from_pretrained(\"ydshieh/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much experience does Charlie Wills have?\"\n",
    "text = \"Charlie Wills is a data scientist with over 10 years experience.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over 10 years'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- extract candidate papers using TF-IDF cosine similarity.\n",
    "- start with small sample of candidate papers.\n",
    "- apply BERT.\n",
    "- if confident method can be used - look into theory and start typing up in dissertation report.\n",
    "\n",
    "Potential approach for TF-IDF cosine similarity:\n",
    "1. Extraction of titles from all JSON documents.\n",
    "2. Getting the list of embeddings for all the titles using BERT.\n",
    "3. Finding the embedding for the input query using BERT\n",
    "4. Using Cosine similarity, to find the list of similar embeddings to that of input query. This generate the list of titles which are similar to the input query. \n",
    "\n",
    "Could extend this to search in the text body after. Recommend starting with a small amount of self-generated text to test approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16196ea7eff63910081d4e10ae1bdb1eb18fd83cb470bb8efbb9fa6b0c724af5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
