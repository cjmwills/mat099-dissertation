{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long should people infected with the virus isolate for? Possible steps to answer this are:\n",
    "- Find all terms that describe isolation (incubation, quarantine, isolation, infectious, transmissable, spreading, transferable, contagious)\n",
    "- Look for all numbers used near these words (in the form 'XX days') that describe the recommend isolation period.\n",
    "- Put all numbers into a list and average them to determine the recommended isolation period.\n",
    "- Spot check a few examples to check it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/clean_pmc.csv\", nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data[data['text'].apply(lambda x: bool(re.match('.*[a-zA-Z]+', x)))]\n",
    "lang = pd.read_csv(\"Data/predicted_lang_10k.csv\", index_col=0).squeeze()\n",
    "data_eng = data_cleaned[lang == 'en']\n",
    "# reset index for use in pd.iterrows()\n",
    "data_eng = data_eng.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_days(tokenized_sent, anchor_word='days', days_regex=\"[0-9.]+\"):\n",
    "    \n",
    "    # find index position of anchor word in sentence\n",
    "    anchor_pos = tokenized_sent.index(anchor_word)\n",
    "    days=[]\n",
    "\n",
    "    # search for the two words preceding the anchor word and check\n",
    "    # if they are numbers (using days_regex to confirm). If so, add\n",
    "    # them to `days` list and take average. This is useful if a range\n",
    "    # has been given. e.g. 2-6 days will return 4 days.\n",
    "    for i in [anchor_pos-2, anchor_pos-1]:\n",
    "        if i >= 0 and bool(re.match(days_regex, tokenized_sent[i])):\n",
    "            day = re.findall(days_regex, tokenized_sent[i])\n",
    "            days.extend(day)\n",
    "\n",
    "    if days == []:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            np.asarray(days, dtype=np.float32).mean()\n",
    "        except:\n",
    "            print(f\"Days list {days} can't be converted to numpy array. Extracted from {tokenized_sent}\")\n",
    "        else:\n",
    "            return np.asarray(days, dtype=np.float32).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommended_isolation(data, keywords, anchor_word='days', subset=None):\n",
    "\n",
    "    # take subset of data to avoid long run time if requested by user\n",
    "    if subset is not None:\n",
    "        data = data[:subset]\n",
    "\n",
    "    isolation=[]\n",
    "    index=[]\n",
    "    keyword_counter = defaultdict(int)\n",
    "    \n",
    "    # loop through pandas dataframe\n",
    "    for indx, row in data.iterrows():\n",
    "        # split 'text' column into sentences\n",
    "        sents = sent_tokenize(row['text'])\n",
    "        # split those sentences into words\n",
    "        words = [word_tokenize(sent) for sent in sents]\n",
    "        # loop through list of lists where outer list is each sentence and\n",
    "        # inner list is each word in that sentence. Convert to lower text\n",
    "        # and remove stopwords. Then extract the no. of days from each sentence\n",
    "        # that contains a keyword(s).\n",
    "        for sent in words:\n",
    "            sent_clean = [word.lower() for word in sent if word.lower() not in stopwords]\n",
    "            keywords_present = [word for word in keywords if word in sent_clean]\n",
    "            if len(keywords_present) > 0 and anchor_word in sent_clean:\n",
    "                days_from_sent = extract_days(sent_clean)\n",
    "                if days_from_sent is not None:\n",
    "                    isolation.append(days_from_sent)\n",
    "                    index.append(indx)\n",
    "                    for keyword_present in keywords_present:\n",
    "                        keyword_counter[keyword_present] += 1\n",
    "            \n",
    "    return isolation, index, keyword_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most similar words to \"quarantine\" from word2vec\n",
    "\n",
    "- 'quarantining', 0.7624921798706055),\n",
    "- 'quarantined', 0.7381011843681335),\n",
    "- 'lockdown', 0.6942132115364075),\n",
    "- 'compulsory', 0.693280816078186),\n",
    "- 'workplace', 0.6875447034835815),\n",
    "- 'visitors', 0.6798210740089417),\n",
    "- 'voluntary', 0.6777964234352112),\n",
    "- 'containment', 0.6706152558326721),\n",
    "- 'tracing', 0.6631444692611694),\n",
    "- 'restrictions', 0.6465995907783508)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['quarantine', 'quarantining', 'quarantined', 'lockdown', 'compulsory', 'workplace']\n",
    "days_to_isolate = recommended_isolation(data_eng, keywords=keywords, subset=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'quarantine': 7, 'quarantining': 2, 'quarantined': 5})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_to_isolate[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median recommended isolation: 14.0 days\n",
      "Number of keywords found: 13\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray(days_to_isolate[0], dtype=np.float32)\n",
    "print(f'Median recommended isolation: {np.median(a)} days')\n",
    "print(f'Number of keywords found: {len(days_to_isolate[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 100.0, 14.0, 4.6, 14.0, 30.0, 5.0, 5.0, 14.0, 14.0, 30.0, 14.0, 3.0]\n",
      "[10, 10, 22, 22, 22, 89, 426, 426, 657, 657, 728, 876, 968]\n"
     ]
    }
   ],
   "source": [
    "print(days_to_isolate[0])\n",
    "print(days_to_isolate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_eng.iloc[1270, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [09:42<00:00,  8.58it/s]  \n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for index, row in tqdm(data_eng[:5000].iterrows(), total=data_eng[:5000].shape[0]):\n",
    "    words = word_tokenize(row['text'])\n",
    "    words_cleaned = [word.lower() for word in words if word.lower() not in stopwords]\n",
    "    corpus.append(words_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quarantining', 0.7624921798706055),\n",
       " ('quarantined', 0.7381011843681335),\n",
       " ('lockdown', 0.6942132115364075),\n",
       " ('compulsory', 0.693280816078186),\n",
       " ('workplace', 0.6875447034835815),\n",
       " ('visitors', 0.6798210740089417),\n",
       " ('voluntary', 0.6777964234352112),\n",
       " ('containment', 0.6706152558326721),\n",
       " ('tracing', 0.6631444692611694),\n",
       " ('restrictions', 0.6465995907783508)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('quarantine')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483cab17ea47bf3dc62612f79753baa304c994dab11b0cb1ba24b5d79c042b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
