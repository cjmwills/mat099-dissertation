{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Data/pdf_json/'\n",
    "filenames = os.listdir(data_dir)\n",
    "\n",
    "papers=[]\n",
    "for filename in filenames:\n",
    "    file = data_dir + filename\n",
    "    paper = json.load(open(file, 'rb'))\n",
    "    body_texts = [body_text['text'] for body_text in paper['body_text']]\n",
    "    text_joined=''\n",
    "    for text in body_texts:\n",
    "        text_joined += text\n",
    "    papers.append(text_joined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    vp3, and vp0 (which is further processed to vp...\n",
       "1    in december 2019, a novel coronavirus, sars-co...\n",
       "2    the 2019-ncov epidemic has spread across china...\n",
       "3    metagenomic sequencing, which allows us to dir...\n",
       "4    infectious bronchitis (ib), which is caused by...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame(papers)[0].str.lower()\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stopwords)\n",
    "data_vectorized = vectorizer.fit_transform(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('累计确诊人数', 73754), ('百万', 73753), ('疫情信息', 73752), ('温州银泰百货商场', 73751), ('武汉肺', 73750), ('武汉爆发', 73749), ('春运', 73748), ('新型冠状病毒', 73747), ('振华量贩聊城闸口店', 73746), ('感冒', 73745)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency = sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1], reverse=True)\n",
    "print(word_frequency[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00772267, 0.00772663, 0.00772267, ..., 0.00772267, 0.00772267,\n",
       "        0.00772293],\n",
       "       [0.00451679, 0.00451672, 0.24398546, ..., 0.00451672, 0.00451672,\n",
       "        0.00451672],\n",
       "       [0.00773239, 0.00773239, 0.00773239, ..., 0.00773239, 0.00773239,\n",
       "        0.00773239],\n",
       "       ...,\n",
       "       [0.00878406, 0.00878406, 0.00878406, ..., 0.04235212, 0.00878406,\n",
       "        0.00878406],\n",
       "       [0.0070009 , 0.0070009 , 0.0070009 , ..., 0.0070009 , 0.0070009 ,\n",
       "        0.0070009 ],\n",
       "       [0.00737976, 0.00737976, 0.04025019, ..., 0.00737976, 0.00737976,\n",
       "        0.00737976]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation()\n",
    "lda.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0: appendicitis 349001 wikidata vhh hd5 idsr crrnas sftsv exo cces\n",
      "\n",
      "Topic #1: bcg g3bp1 bees atv reovirus lncrnas aceis avan lpv hdl\n",
      "\n",
      "Topic #2: lecb vp35 uccs bleomycin adm1 fmw nelfinavir mtase muc5ac g64s\n",
      "\n",
      "Topic #3: mirnas tlr5 hyposmia top3b meplazumab dcv cpac parhyale msti pscnv\n",
      "\n",
      "Topic #4: preprint medrxiv doi covid license patients cases 19 2020 10\n",
      "\n",
      "Topic #5: cr3022 pr8 mad1 parp2 cml mad2 phi6 drv muc4 nsp1a\n",
      "\n",
      "Topic #6: dux4 entrying trim25 mxb valinomycin phosphorus parkin c00422 vadr vt\n",
      "\n",
      "Topic #7: osd nab virosal ccfr ami nabs symptoma denv4 isd sensr\n",
      "\n",
      "Topic #8: curvature hsa agi prf eps8 enc dpcr klk13 egcg mir\n",
      "\n",
      "Topic #9: sox cd147 vl masp cosmetic gn lns sre 20021493 trisilix\n"
     ]
    }
   ],
   "source": [
    "# taken from https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles#Latend-Dirichlet-Allocation\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    message = \"\\nTopic #%d: \" % topic_idx\n",
    "    message += \" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove non-english words to make sense of output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483cab17ea47bf3dc62612f79753baa304c994dab11b0cb1ba24b5d79c042b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
